{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/wdgstl/GCPE/pefirms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_firms = df[(df['country'].str.lower() == 'united states') & (df['website'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joingardencity.com'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = us_firms.iloc[1]\n",
    "\n",
    "website = record['website']\n",
    "\n",
    "website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                             united states\n",
       "founded                                      2020\n",
       "id                                   sugarcapital\n",
       "industry         venture capital & private equity\n",
       "linkedin_url    linkedin.com/company/sugarcapital\n",
       "locality                            san francisco\n",
       "name                                sugar capital\n",
       "region                                 california\n",
       "size                                         1-10\n",
       "website                              sugarcap.com\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_firms.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3716"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def read_txt(folder_path, filename):\n",
    "    with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "    return content\n",
    "\n",
    "text = read_txt('/Users/wdgstl/GCPE/scraped_pages', 'joingardencity_com_relevant.txt')\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry Focus Areas and Specific Theses:\n",
      "- HVAC:\n",
      "  - No specific thesis areas mentioned.\n",
      "- Restaurant remodeling:\n",
      "  - No specific thesis areas mentioned.\n",
      "- Pool construction:\n",
      "  - No specific thesis areas mentioned.\n",
      "- Businesses with thriving cultures:\n",
      "  - Long-term investment focus\n",
      "  - People and purpose prioritized over short-term returns\n",
      "  - Little to no debt utilized in investments\n",
      "  - Partnering with existing leaders rather than replacing them\n",
      "  - Preservation and honoring of founders' legacies\n",
      "  - Protection and further investment in thriving cultures.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def call_mixtral(prompt):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"mixtral\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# Example use\n",
    "prompt = f\"\"\"\n",
    "You are reviewing a private equity firm's website description to identify its explicitly stated investment areas.\n",
    "\n",
    "Input text:\n",
    "{text}\n",
    "\n",
    "Your task is to extract only the information that is clearly and explicitly stated. Do **not** infer or assume investment focus based on context, vague language, employee backgrounds, or general descriptions. If the industry or thesis is not **directly stated** in the text, you must omit it.\n",
    "\n",
    "Instructions:\n",
    "1. Identify and list only the **explicitly stated industry focus areas**â€”industries the firm directly says it invests in.\n",
    "2. For each industry, extract any **specific investment thesis areas** (e.g., types of businesses, value creation strategies, market conditions, subsectors) that are **directly mentioned**.\n",
    "\n",
    "Important:\n",
    "- Ignore implications, suggestions, or inferred meanings.\n",
    "- Do not include areas just because they appear in example deals or portfolios unless the firm explicitly states them as a focus.\n",
    "- Do not include boilerplate descriptions or general language unless tied to a named industry.\n",
    "\n",
    "Format your response exactly like this:\n",
    "Industry Focus Areas and Specific Theses:\n",
    "- [Industry 1]\n",
    "  - Thesis Area(s): \n",
    "    - [Bullet point]\n",
    "    - [Bullet point]\n",
    "- [Industry 2]\n",
    "  - Thesis Area(s): \n",
    "    - [Bullet point]\n",
    "    - [Bullet point]\n",
    "(...continue for all explicitly stated industries)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(call_mixtral(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://blackstone.com\n",
      "Visiting: https://blackstone.com#primary\n",
      "Visiting: https://www.blackstone.com/\n",
      "Visiting: https://blackstone.com/the-firm/\n",
      "Visiting: https://blackstone.com/our-clients/overview/\n",
      "Visiting: https://blackstone.com/financial-advisors/\n",
      "Visiting: https://blackstone.com/family-offices-endowments-foundations/\n",
      "Visiting: https://www.blackstone.com/our-businesses/credit-and-insurance-bxci/#Insurance\n",
      "Visiting: https://blackstone.com/our-impact/building-sustainable-businesses/\n",
      "Visiting: https://blackstone.com/our-impact/blackstone-charitable-foundation/\n",
      "Visiting: https://www.blackstone.com/our-impact/blackstone-launchpad/\n",
      "Visiting: https://blackstone.com/our-businesses/private-equity/\n",
      "Visiting: https://blackstone.com/our-businesses/real-estate/\n",
      "Visiting: https://blackstone.com/our-businesses/credit-and-insurance-bxci/\n",
      "Visiting: https://blackstone.com/our-businesses/blackstone-multi-asset-investing-bxma/\n",
      "Visiting: https://blackstone.com/our-businesses/strategic-partners/\n",
      "Visiting: https://blackstone.com/our-businesses/tactical-opportunities/\n",
      "Visiting: https://blackstone.com/our-businesses/infrastructure/\n",
      "Visiting: https://blackstone.com/our-businesses/life-sciences/\n",
      "Visiting: https://blackstone.com/our-businesses/blackstone-growth-bxg/\n",
      "Visiting: https://blackstone.com/our-businesses/blackstone-energy-transition-partners/\n",
      "Visiting: https://www.blackstone.com/our-businesses/technology-and-innovations/\n",
      "Visiting: https://blackstone.com/insights/\n",
      "Visiting: https://blackstone.com/insights/market-views\n",
      "Visiting: https://www.blackstone.com/pws/essentials-of-private-markets/\n",
      "Visiting: https://www.blackstone.com/pws/essentials-of-private-equity/\n",
      "Visiting: https://www.blackstone.com/pws/essentials-of-private-credit/\n",
      "Visiting: https://www.blackstone.com/pws/essentials-of-private-real-estate/\n",
      "Visiting: https://www.blackstone.com/pws/essentials-of-private-infrastructure/\n",
      "Visiting: https://www.blackstone.com/pattern-recognition/\n",
      "Relevant content written to: scraped_pages/blackstone_com_relevant.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Keywords to skip crawling\n",
    "EXCLUDE_KEYWORDS = {\n",
    "    \"careers\", \"career\", \"jobs\", \"team\", \"people\",\n",
    "    \"privacy\", \"terms\", \"legal\",\n",
    "    \"contact\", \"support\", \"help\", \"faq\",\n",
    "    \"blog\", \"news\", \"press\", \"media\",\n",
    "    \"events\", \"webinar\", \"culture\",\n",
    "    \"login\", \"signup\", \"register\", \"subscribe\",\n",
    "    \"cookie\", \"rss\", \"sitemap\",\n",
    "    \"leadership\", \".pdf\", \".jpg\", \".jpeg\",\n",
    "    \"branch\", \".xlsx\", \"email\", \"article\", \"report\"\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_DIR = \"scraped_pages\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize local embedding model (free)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def crawl_site(start_url, max_pages=1000):\n",
    "    visited = set()\n",
    "    to_visit = [start_url]\n",
    "    base_domain = urlparse(start_url).netloc.replace(\"www.\", \"\")\n",
    "    firm_name = base_domain.replace('.', '_')\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{firm_name}.txt\")\n",
    "\n",
    "    # Headless browser setup (disable images, CSS, fonts for speed)\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_experimental_option(\"prefs\", {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.fonts\": 2,\n",
    "    })\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        print(\"Visiting:\", url)\n",
    "\n",
    "        root = url.split(\"#\")[0]\n",
    "        driver.get(root)\n",
    "        try:\n",
    "            WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        parsed = urlparse(url)\n",
    "\n",
    "        # Extract paragraphs from full page or in-page fragment\n",
    "        if parsed.fragment:\n",
    "            section = soup.find(id=parsed.fragment)\n",
    "            paras = section.find_all(\"p\") if section else []\n",
    "        else:\n",
    "            paras = soup.find_all(\"p\")\n",
    "\n",
    "        # Append paragraphs to output file\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            for p in paras:\n",
    "                text = p.get_text(separator=\" \", strip=True)\n",
    "                if text:\n",
    "                    f.write(text + \"\\n\\n\")\n",
    "\n",
    "        # Enqueue new links found in the page\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = urljoin(root, tag[\"href\"])\n",
    "            p2 = urlparse(href)\n",
    "            root_link = p2._replace(query=\"\", fragment=\"\").geturl()\n",
    "            frag = p2.fragment\n",
    "\n",
    "            # Same-site check (allow www and non-www)\n",
    "            if p2.netloc.replace(\"www.\", \"\") != base_domain:\n",
    "                continue\n",
    "            # Skip noisy or irrelevant paths\n",
    "            if any(kw in root_link.lower() for kw in EXCLUDE_KEYWORDS):\n",
    "                continue\n",
    "\n",
    "            next_url = root_link + (f\"#{frag}\" if frag else \"\")\n",
    "            if next_url not in visited and next_url not in to_visit:\n",
    "                to_visit.append(next_url)\n",
    "\n",
    "    driver.quit()\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def embed_and_rank_paragraphs(paragraphs, query, top_k=10, min_words=5, min_chars=60, boost_weight=0.2):\n",
    "    \"\"\"\n",
    "    Embed and rank paragraphs by semantic similarity to the query,\n",
    "    then boost those containing key investment keywords.\n",
    "    Filters out very short, heading-like, or excessively brief text blocks.\n",
    "    \"\"\"\n",
    "    # Expanded investment-related keywords to boost\n",
    "    KEYWORDS = {\n",
    "        \"focus\", \"invest\", \"strategy\", \"portfolio\", \"sector\", \"thesis\",\n",
    "        \"acquire\", \"acquires\", \"grows\", \"grow\", \"business\", \"company\", \"holding\", \"model\", \"mission\", \"goal\"\n",
    "    }\n",
    "\n",
    "    def is_noise(p):\n",
    "        if len(p.split()) < min_words:\n",
    "            return True\n",
    "        if len(p) < min_chars:\n",
    "            return True\n",
    "        letters = [c for c in p if c.isalpha()]\n",
    "        if letters:\n",
    "            upper_ratio = sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "            if upper_ratio > 0.6:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    clean = [p for p in paragraphs if not is_noise(p)]\n",
    "    if not clean:\n",
    "        clean = paragraphs  # fallback\n",
    "\n",
    "    # compute embeddings\n",
    "    query_vec = model.encode(query, convert_to_numpy=True)\n",
    "    para_embs = model.encode(clean, convert_to_numpy=True, batch_size=100)\n",
    "\n",
    "    # cosine similarities\n",
    "    sims = np.dot(para_embs, query_vec) / (\n",
    "        np.linalg.norm(para_embs, axis=1) * np.linalg.norm(query_vec)\n",
    "    )\n",
    "\n",
    "    # keyword flags and combined scores\n",
    "    keyword_flags = np.array([1 if any(kw in p.lower() for kw in KEYWORDS) else 0 for p in clean])\n",
    "    combined = sims + boost_weight * keyword_flags\n",
    "\n",
    "    # sort by combined score and return only text and score\n",
    "    idxs = np.argsort(combined)[::-1][:top_k]\n",
    "    return [(clean[i], float(combined[i])) for i in idxs]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://blackstone.com\"\n",
    "    txt_file = crawl_site(url, max_pages=30)\n",
    "\n",
    "    # load paragraphs and deduplicate\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_paras = [p.strip() for p in f.read().split(\"\\n\\n\") if p.strip()]\n",
    "    seen = set()\n",
    "    paragraphs = []\n",
    "    for p in raw_paras:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            paragraphs.append(p)\n",
    "\n",
    "    # derive display name from URL\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.replace(\"www.\", \"\")\n",
    "    display_name = domain.split('.')[0].replace('-', ' ').replace('_', ' ').title()\n",
    "\n",
    "    query = f\"{display_name} private equity industry focus areas, investment model, and corresponding investment thesis statements.\"\n",
    "    top = embed_and_rank_paragraphs(paragraphs, query, top_k=60)\n",
    "\n",
    "    # Delete raw scraped paragraphs file\n",
    "    try:\n",
    "        os.remove(txt_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Write top relevant paragraphs to new file (without scores)\n",
    "    firm_name = domain.replace('.', '_')\n",
    "    relevant_file = os.path.join(OUTPUT_DIR, f\"{firm_name}_relevant.txt\")\n",
    "    with open(relevant_file, \"w\", encoding=\"utf-8\") as rf:\n",
    "        for text, _ in top:\n",
    "            rf.write(text + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Relevant content written to: {relevant_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'bansk group', 'banskgroup.com', 'test')\n",
      "(2, 'garden city companies', 'joingardencity.com', 'test')\n",
      "(3, 'sileo capital', 'sileocapital.com', 'test')\n",
      "(4, 'intonation ventures', 'intonationventures.com', 'test')\n",
      "(5, 'sugar capital', 'sugarcap.com', 'test')\n",
      "(6, 'fuul capital', 'fuulcapital.com', 'test')\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT * FROM firms;\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: bansk group\n",
      "Saved: garden city companies\n",
      "Saved: sileo capital\n",
      "Saved: intonation ventures\n",
      "Saved: sugar capital\n",
      "Saved: fuul capital\n"
     ]
    }
   ],
   "source": [
    "#want to do this in paralell\n",
    "for i in range(len(us_firms)):\n",
    "    name = us_firms.iloc[i]['name']\n",
    "    website = us_firms.iloc[i]['website']\n",
    "    save_firm_to_db(name, website, 'test')\n",
    "    # crawl_website(name, f'https://www.{website}')\n",
    "    if i == 5:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'bansk group', 'banskgroup.com', 'test')\n",
      "(2, 'garden city companies', 'joingardencity.com', 'test')\n",
      "(3, 'sileo capital', 'sileocapital.com', 'test')\n",
      "(4, 'intonation ventures', 'intonationventures.com', 'test')\n",
      "(5, 'sugar capital', 'sugarcap.com', 'test')\n",
      "(6, 'fuul capital', 'fuulcapital.com', 'test')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM firms\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
